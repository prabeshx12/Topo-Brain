{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2219f7d5",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a370e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from config import get_default_config, get_fast_config\n",
    "from preprocessing import MRIPreprocessor\n",
    "from dataset import BrainMRIDataset, create_data_loaders\n",
    "from utils import (\n",
    "    setup_logging,\n",
    "    discover_dataset,\n",
    "    create_patient_level_split,\n",
    "    visualize_sample,\n",
    "    set_random_seeds,\n",
    ")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# For better plots\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6ca5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "# Use 'fast' config for quick experimentation\n",
    "config = get_fast_config()\n",
    "\n",
    "# Or use default for full resolution\n",
    "# config = get_default_config()\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  Data root: {config.data.data_root}\")\n",
    "print(f\"  Output root: {config.data.output_root}\")\n",
    "print(f\"  Target spacing: {config.preprocessing.target_spacing}\")\n",
    "print(f\"  Batch size: {config.training.batch_size}\")\n",
    "print(f\"  Random seed: {config.split.random_seed}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "set_random_seeds(config.split.random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9c57d3",
   "metadata": {},
   "source": [
    "## 2. Dataset Discovery\n",
    "\n",
    "Discover all MRI volumes in the BIDS-formatted dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c0c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover dataset\n",
    "data_list = discover_dataset(config.data.data_root, config.data)\n",
    "\n",
    "print(f\"\\nFound {len(data_list)} volumes\")\n",
    "print(f\"\\nFirst 5 entries:\")\n",
    "for i, item in enumerate(data_list[:5]):\n",
    "    print(f\"{i+1}. {item['subject']}/{item['session']}/{item['modality']} - {item['field_strength']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e8ae15",
   "metadata": {},
   "source": [
    "## 3. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59fd052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dataset composition\n",
    "import pandas as pd\n",
    "\n",
    "# Create dataframe for analysis\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'subject': item['subject'],\n",
    "        'session': item['session'],\n",
    "        'modality': item['modality'],\n",
    "        'field_strength': item['field_strength'],\n",
    "    }\n",
    "    for item in data_list\n",
    "])\n",
    "\n",
    "print(\"Dataset Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total volumes: {len(df)}\")\n",
    "print(f\"\\nSubjects: {df['subject'].nunique()}\")\n",
    "print(df['subject'].unique())\n",
    "print(f\"\\nSessions: {df['session'].nunique()}\")\n",
    "print(df['session'].value_counts())\n",
    "print(f\"\\nModalities: {df['modality'].nunique()}\")\n",
    "print(df['modality'].value_counts())\n",
    "print(f\"\\nField Strengths:\")\n",
    "print(df['field_strength'].value_counts())\n",
    "\n",
    "# Cross-tabulation\n",
    "print(\"\\nCross-tabulation (Session x Modality):\")\n",
    "print(pd.crosstab(df['session'], df['modality']))\n",
    "\n",
    "print(\"\\nCross-tabulation (Subject x Modality):\")\n",
    "print(pd.crosstab(df['subject'], df['modality']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a858abf",
   "metadata": {},
   "source": [
    "## 4. Patient-Level Splitting\n",
    "\n",
    "Create train/val/test splits ensuring no data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753c3d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create patient-level split\n",
    "train_data, val_data, test_data = create_patient_level_split(\n",
    "    data_list,\n",
    "    train_ratio=config.split.train_ratio,\n",
    "    val_ratio=config.split.val_ratio,\n",
    "    test_ratio=config.split.test_ratio,\n",
    "    random_seed=config.split.random_seed,\n",
    ")\n",
    "\n",
    "# Verify splits\n",
    "train_subjects = set(item['subject'] for item in train_data)\n",
    "val_subjects = set(item['subject'] for item in val_data)\n",
    "test_subjects = set(item['subject'] for item in test_data)\n",
    "\n",
    "print(f\"\\nSplit Verification:\")\n",
    "print(f\"Train subjects: {sorted(train_subjects)}\")\n",
    "print(f\"Val subjects: {sorted(val_subjects)}\")\n",
    "print(f\"Test subjects: {sorted(test_subjects)}\")\n",
    "\n",
    "# Check for leakage\n",
    "assert len(train_subjects & val_subjects) == 0, \"Train-Val leakage!\"\n",
    "assert len(train_subjects & test_subjects) == 0, \"Train-Test leakage!\"\n",
    "assert len(val_subjects & test_subjects) == 0, \"Val-Test leakage!\"\n",
    "print(\"\\n✓ No data leakage detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e3094e",
   "metadata": {},
   "source": [
    "## 5. Preprocessing Pipeline\n",
    "\n",
    "Preprocess a single volume to demonstrate the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71dc455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample to preprocess\n",
    "sample_idx = 0\n",
    "sample_path = data_list[sample_idx]['image']\n",
    "\n",
    "print(f\"Preprocessing sample: {sample_path.name}\")\n",
    "print(f\"Subject: {data_list[sample_idx]['subject']}\")\n",
    "print(f\"Session: {data_list[sample_idx]['session']}\")\n",
    "print(f\"Modality: {data_list[sample_idx]['modality']}\")\n",
    "print(f\"Field strength: {data_list[sample_idx]['field_strength']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a91f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = MRIPreprocessor(config.preprocessing)\n",
    "\n",
    "# Preprocess single volume\n",
    "output_path = config.data.output_root / f\"test_{sample_path.name}\"\n",
    "\n",
    "result = preprocessor.preprocess_single(\n",
    "    sample_path,\n",
    "    output_path,\n",
    "    save_intermediate=True,  # Save intermediate steps\n",
    ")\n",
    "\n",
    "print(\"\\nPreprocessing Results:\")\n",
    "print(f\"Original shape: {result['original_shape']}\")\n",
    "print(f\"Processed shape: {result['processed_shape']}\")\n",
    "print(f\"\\nStatistics:\")\n",
    "for key, value in result['statistics'].items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa452d",
   "metadata": {},
   "source": [
    "## 6. DataLoader Creation\n",
    "\n",
    "Create PyTorch DataLoaders for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058a9818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this demo, use raw data (skip preprocessing for speed)\n",
    "# In production, use preprocessed data\n",
    "\n",
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "    config, train_data, val_data, test_data\n",
    ")\n",
    "\n",
    "print(f\"DataLoaders created:\")\n",
    "print(f\"  Train: {len(train_loader.dataset)} samples, {len(train_loader)} batches\")\n",
    "print(f\"  Val: {len(val_loader.dataset)} samples, {len(val_loader)} batches\")\n",
    "print(f\"  Test: {len(test_loader.dataset)} samples, {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c313b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a batch\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(f\"Batch contents:\")\n",
    "print(f\"  Images shape: {batch['image'].shape}\")\n",
    "print(f\"  Images dtype: {batch['image'].dtype}\")\n",
    "print(f\"  Images range: [{batch['image'].min():.4f}, {batch['image'].max():.4f}]\")\n",
    "print(f\"  Subjects: {batch['subject']}\")\n",
    "print(f\"  Sessions: {batch['session']}\")\n",
    "print(f\"  Modalities: {batch['modality']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0057318",
   "metadata": {},
   "source": [
    "## 7. Visualization\n",
    "\n",
    "Visualize samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff76a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample from the batch\n",
    "sample_idx = 0\n",
    "image = batch['image'][sample_idx, 0].cpu().numpy()  # Remove channel dim\n",
    "subject = batch['subject'][sample_idx]\n",
    "\n",
    "# Get middle slices\n",
    "mid_sag = image.shape[0] // 2\n",
    "mid_cor = image.shape[1] // 2\n",
    "mid_ax = image.shape[2] // 2\n",
    "\n",
    "# Compute intensity range for display\n",
    "if image[image != 0].size > 0:\n",
    "    vmin, vmax = np.percentile(image[image != 0], [1, 99])\n",
    "else:\n",
    "    vmin, vmax = image.min(), image.max()\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(image[mid_sag, :, :].T, cmap='gray', vmin=vmin, vmax=vmax, origin='lower')\n",
    "axes[0].set_title(f'Sagittal (x={mid_sag})')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(image[:, mid_cor, :].T, cmap='gray', vmin=vmin, vmax=vmax, origin='lower')\n",
    "axes[1].set_title(f'Coronal (y={mid_cor})')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(image[:, :, mid_ax].T, cmap='gray', vmin=vmin, vmax=vmax, origin='lower')\n",
    "axes[2].set_title(f'Axial (z={mid_ax})')\n",
    "axes[2].axis('off')\n",
    "\n",
    "fig.suptitle(f'Subject: {subject}', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eb0545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize intensity distribution\n",
    "nonzero_values = image[image != 0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(nonzero_values.flatten(), bins=100, alpha=0.7, color='blue')\n",
    "axes[0].set_title('Intensity Distribution')\n",
    "axes[0].set_xlabel('Intensity')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(nonzero_values.flatten())\n",
    "axes[1].set_title('Intensity Box Plot')\n",
    "axes[1].set_ylabel('Intensity')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Statistics:\")\n",
    "print(f\"  Mean: {nonzero_values.mean():.4f}\")\n",
    "print(f\"  Std: {nonzero_values.std():.4f}\")\n",
    "print(f\"  Min: {nonzero_values.min():.4f}\")\n",
    "print(f\"  Max: {nonzero_values.max():.4f}\")\n",
    "print(f\"  Median: {np.median(nonzero_values):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cc6a1f",
   "metadata": {},
   "source": [
    "## 8. Statistics Analysis\n",
    "\n",
    "Compute dataset-wide statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd909ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import compute_dataset_statistics\n",
    "\n",
    "# Compute statistics on training set\n",
    "print(\"Computing training set statistics...\")\n",
    "train_stats = compute_dataset_statistics(train_loader, max_samples=10)\n",
    "\n",
    "print(\"\\nTraining Set Statistics:\")\n",
    "for key, value in train_stats.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4693ebc",
   "metadata": {},
   "source": [
    "## 9. Demo Training Loop\n",
    "\n",
    "Demonstrate how to use the DataLoader in a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1403811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple demo training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "num_epochs = 1\n",
    "num_batches_per_epoch = 3  # Limit for demo\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        if batch_idx >= num_batches_per_epoch:\n",
    "            break\n",
    "        \n",
    "        # Get data\n",
    "        images = batch['image'].to(device)\n",
    "        subjects = batch['subject']\n",
    "        \n",
    "        # Simulate forward pass (just compute mean)\n",
    "        output = images.mean(dim=[2, 3, 4])  # Global average pooling\n",
    "        \n",
    "        # Simulate loss computation\n",
    "        dummy_loss = output.mean()\n",
    "        \n",
    "        print(f\"  Batch {batch_idx + 1}:\")\n",
    "        print(f\"    Shape: {images.shape}\")\n",
    "        print(f\"    Subjects: {subjects}\")\n",
    "        print(f\"    Loss: {dummy_loss.item():.4f}\")\n",
    "        \n",
    "        # Check for NaN/Inf\n",
    "        if torch.isnan(images).any():\n",
    "            print(\"    WARNING: NaN detected!\")\n",
    "        if torch.isinf(images).any():\n",
    "            print(\"    WARNING: Inf detected!\")\n",
    "\n",
    "print(\"\\n✓ Training loop demo completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e932dfd4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✅ Dataset discovery and exploration\n",
    "2. ✅ Patient-level splitting (no data leakage)\n",
    "3. ✅ Preprocessing pipeline\n",
    "4. ✅ DataLoader creation\n",
    "5. ✅ Visualization and statistics\n",
    "6. ✅ Training loop integration\n",
    "\n",
    "### Next Steps:\n",
    "- Run full preprocessing: `python example_pipeline.py --preprocess`\n",
    "- Implement your model architecture\n",
    "- Customize augmentation strategies\n",
    "- Generate proper brain masks using HD-BET or SynthStrip\n",
    "- Experiment with different normalization methods\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Always use patient-level splits** to prevent data leakage\n",
    "- **Set random seeds** for reproducibility\n",
    "- **Monitor statistics** to ensure proper normalization\n",
    "- **Visualize samples** to verify preprocessing\n",
    "- **Save split information** for reproducibility"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
